{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain neo4j sentence-transformers llama-index openai\n",
    "%pip install pandas numpy\n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom OpenAI API: using deepbricks API\n",
    "\n",
    "# Configure Deepbricks API and GPT-4o-mini model\n",
    "BASE_URL = \"https://api.deepbricks.ai/v1/chat/completions\"\n",
    "API_KEY = \"sk-WnstQIiLBZkwq17zqjYIOhMIOJUVNfbWCxlvOjmPXZnCDVPA\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "temperature = 0.7\n",
    "max_tokens = 150\n",
    "\n",
    "class CustomOpenAI:\n",
    "    def __init__(self, base_url, api_key, model):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            'Authorization': f'Bearer {api_key}',\n",
    "            'Content-Type': 'application/json',\n",
    "        }\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self,messages, tem=temperature, max_t=max_tokens):\n",
    "        payload = {\n",
    "            'model': self.model,  # add model name\n",
    "            'messages': messages,  # send messages as context\n",
    "            'temperature': tem, # add temperature\n",
    "            'max_tokens': max_t # add max tokens\n",
    "        }\n",
    "\n",
    "        response = requests.post(\n",
    "            f'{self.base_url}',\n",
    "            headers=self.headers,\n",
    "            json=payload\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()['choices'][0]['message']['content'].strip()\n",
    "        else:\n",
    "            raise Exception(f\"API call failed: {response.status_code}, {response.text}\")\n",
    "\n",
    "# using Custom OpenAI API class\n",
    "llm = CustomOpenAI(base_url=BASE_URL, api_key=API_KEY, model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Ollama Local API\n",
    "\n",
    "# Configure Ollama Local API and  model\n",
    "# https://github.com/ollama/ollama/blob/main/docs/openai.md\n",
    "BASE_URL = \"http://localhost:11434/api/generate\"\n",
    "API_KEY = \"sk-WnstQIiLBZkwq17zqjYIOhMIOJUVNfbWCxlvOjmPXZnCDVPA\"\n",
    "MODEL_NAME = \"llama3.1:latest\"\n",
    "temperature = 0.7\n",
    "max_tokens = 150\n",
    "\n",
    "class OllamaLLM:\n",
    "    def __init__(self, model, base_url):\n",
    "        self.model_name = model\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def generate(self, prompt, temperature=0.7, max_tokens=150):\n",
    "        url = f\"{self.base_url}\"\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"text\"]\n",
    "        else:\n",
    "            raise Exception(f\"Error in Ollama API: {response.status_code}, {response.text}\")\n",
    "\n",
    "# initialize Ollama LLMs\n",
    "llm = OllamaLLM(model=MODEL_NAME,base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T10:55:13.764732Z",
     "start_time": "2024-10-03T10:55:13.761510Z"
    }
   },
   "source": [
    "### LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "local_llm = \"llama3.2:latest\"\n",
    "base_url = \"http://localhost:11434/api/generate\"\n",
    "llm = ChatOllama(model=local_llm, base_url, temperature=0.7,num_predict = 256)\n",
    "llm_json_mode = ChatOllama(model=local_llm, base_url, temperature=0.7, format=\"json\")\n"
   ],
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (3275993531.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[1], line 6\u001B[0;36m\u001B[0m\n\u001B[0;31m    llm = ChatOllama(model=local_llm, base_url, temperature=0.7,num_predict = 256)\u001B[0m\n\u001B[0m                                                                                 ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 CSV 文件\n",
    "data = pd.read_csv('test-data-2.csv')\n",
    "\n",
    "# Replace NaN values with empty strings for Neo4j compatibility\n",
    "data = data.replace({np.nan: ''})\n",
    "\n",
    "# 连接到 Neo4j\n",
    "driver = GraphDatabase.driver(\"neo4j://localhost:7687\", auth=(\"neo4j\", \"test_password\"))\n",
    "\n",
    "# 创建服装节点以及它们之间的关系, 创建Neo4j中的图节点和关系\n",
    "def create_clothing_graph(tx, row):\n",
    "    query = (\n",
    "        \"\"\"\n",
    "        MERGE (c:Clothing {id: $id, name: $name, brand: $brand, type: $type, group: $group, details: $details, \n",
    "                           price: $price, currency: $currency, color: $color, size: $size, \n",
    "                           style: $style, pattern: $pattern, material: $material, occasion: $occasion})\n",
    "        MERGE (b:Brand {name: $brand})\n",
    "        MERGE (t:Type {name: $type})\n",
    "        MERGE (b)-[:MAKES]->(c)\n",
    "        MERGE (c)-[:BELONGS_TO]->(t)\n",
    "        \"\"\"\n",
    "    )\n",
    "    tx.run(query, id=row['id'], name=row['name'], brand=row['brand'], type=row['type'], group=row['group'], \n",
    "           details=row['details'], price=row['price'], currency=row['currency'], color=row['color'], \n",
    "           size=row['size'], style=row['style'], pattern=row['pattern'], material=row['material'], \n",
    "           occasion=row['occasion'])\n",
    "\n",
    "# 将数据导入 Neo4j 并建立图\n",
    "with driver.session() as session:\n",
    "    data.apply(lambda row: session.execute_write(create_clothing_graph, row), axis=1)\n",
    "\n",
    "# 删除全文索引\n",
    "def delete_fulltext_index():\n",
    "    with driver.session() as session:\n",
    "        session.run(\"DROP INDEX clothingIndex IF EXISTS\")\n",
    "\n",
    "# 创建全文索引\n",
    "def create_fulltext_index():\n",
    "    with driver.session() as session:\n",
    "        session.run(\"\"\"\n",
    "        CREATE FULLTEXT INDEX clothingIndex\n",
    "        FOR (c:Clothing)\n",
    "        ON EACH [c.id, c.name, c.brand, c.details]\n",
    "        \"\"\")\n",
    "\n",
    "# 在服装节点创建后调用删除索引和创建索引函数\n",
    "delete_fulltext_index()  # 先删除索引（如果存在）\n",
    "create_fulltext_index()  # 然后创建新的索引\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/var/folders/ml/k0q5h5795wn24p71_43s10xw0000gn/T/ipykernel_53322/2864131403.py:15: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(store_embedding, row['name'], embeddings[i])\n"
     ]
    }
   ],
   "source": [
    "# 加载嵌入模型\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 生成每个服装的描述嵌入向量\n",
    "data['description'] = data[['name', 'brand', 'type', 'details']].apply(lambda x: ' '.join(x), axis=1)\n",
    "embeddings = model.encode(data['description'].tolist())\n",
    "\n",
    "# 存储嵌入到 Neo4j\n",
    "def store_embedding(tx, name, embedding):\n",
    "    query = \"MATCH (c:Clothing {name: $name}) SET c.embedding = $embedding\"\n",
    "    tx.run(query, name=name, embedding=embedding.tolist())\n",
    "\n",
    "with driver.session() as session:\n",
    "    for i, row in data.iterrows():\n",
    "        session.write_transaction(store_embedding, row['name'], embeddings[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义一个函数，从 Neo4j 中加载所有服装的嵌入向量\n",
    "# def load_all_embeddings(tx):\n",
    "#     result = tx.run(\"MATCH (c:Clothing) RETURN c.name, c.embedding\")\n",
    "#     # 遍历查询结果并打印每一条记录\n",
    "#     for record in result:\n",
    "#         print(f\"Name: {record['c.name']}, Embedding: {record['c.embedding']}\")\n",
    "#     return {record['c.name']: np.array(record['c.embedding']) for record in result}\n",
    "    \n",
    "# 定义一个函数进行全文检索\n",
    "def retrieve_products_fulltext(query):\n",
    "    with driver.session() as session:\n",
    "        # 使用全文索引在 Clothing 节点中查找匹配的节点\n",
    "        result = session.run(\"\"\"\n",
    "        CALL db.index.fulltext.queryNodes('clothingIndex', $query)\n",
    "        YIELD node, score\n",
    "        RETURN node.id as id, node.name AS name, node.brand AS brand, node.details AS details, score\n",
    "        ORDER BY score DESC LIMIT 10\n",
    "        \"\"\", {\"query\": query})\n",
    "        \n",
    "        # 打印每一个匹配的结果\n",
    "        products = []\n",
    "        for record in result:\n",
    "            \n",
    "            print(f\"ID: {record['id']}, Name: {record['name']}, Brand: {record['brand']}, Details: {record['details']}, Score: {record['score']}\")\n",
    "            \n",
    "            products.append({\n",
    "                \"id\": record[\"id\"],  # 包含 ID\n",
    "                \"name\": record[\"name\"],\n",
    "                \"brand\": record[\"brand\"],\n",
    "                \"details\": record[\"details\"],\n",
    "                \"score\": record[\"score\"]\n",
    "            })\n",
    "\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义提示模板\n",
    "template = \"\"\"\n",
    "User is searching for clothing. The input query is: \"{input_query}\".\n",
    "The following product data matches the query: \n",
    "{product_data}.\n",
    "Generate a response to suggest clothing.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"input_query\", \"product_data\"], template=template)\n",
    "\n",
    "# 自定义生成响应函数\n",
    "def generate_response(openai_model, input_query, product_data):\n",
    "    combined_prompt = \"\"\"\n",
    "User is searching for clothing. The input query is: \"{}\".\n",
    "The following product data matches the query:\n",
    "{}.\n",
    "Generate a response to suggest clothing.\n",
    "\"\"\".format(input_query, product_data)\n",
    "\n",
    "    # messages = [\n",
    "    #     {\"role\": \"system\", \"content\": \"You are a helpful fashion assistant\"},\n",
    "    #     {\"role\": \"user\", \"content\": combined_prompt}\n",
    "    # ]\n",
    "\n",
    "    # Ollama API expects a single string for the prompt\n",
    "    # OllamaPrompt = f\"You are a helpful fashion assistant.\\n{combined_prompt}\"\n",
    "\n",
    "    response = openai_model.generate(combined_prompt)  # 修改为传递messages\n",
    "    # 输出响应内容以检查是否为有效的 JSON\n",
    "    print(\"Raw API Response:\", response)\n",
    "\n",
    "    # response = openai_model.generate(messages)  # 修改为传递messages\n",
    "\n",
    "    try:\n",
    "        # 尝试将响应解析为 JSON\n",
    "        response_json = response.json()\n",
    "        return response_json[\"text\"]\n",
    "    except ValueError as e:\n",
    "        # 如果解析失败，捕获异常并打印原始响应内容\n",
    "        print(f\"Error decoding JSON: {e}. Raw response: {response.text}\")\n",
    "        return response.text  # 如果不是 JSON，则返回原始文本\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主题关键词\n",
    "# TODO: Add more keywords to improve topic detection accuracy\n",
    "on_topic_keywords = ['jeans', 'dress', 'shirt', 'pants', 'clothing', 'size', 'color', 'fashion', 'outfit', 'trousers', 'jacket']\n",
    "\n",
    "# TODO: \n",
    "off_topic_keywords = ['weather', 'news', 'movie', 'sports', 'politics', 'celebrity', 'tv show', 'music', 'event']\n",
    "\n",
    "def detect_topic(user_input):\n",
    "    if any(word in user_input.lower() for word in on_topic_keywords):\n",
    "        return \"on_topic\"\n",
    "    elif any(word in user_input.lower() for word in off_topic_keywords):\n",
    "        return \"off_topic\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 55, Name: Casual Shirt, Brand: HuiLi, Details: Cotton shirt with button-down collar, Score: 3.6248295307159424\n",
      "ID: 35, Name: Casual T-shirt, Brand: HuiLi, Details: Cotton t-shirt with round neck, Score: 3.4255385398864746\n",
      "ID: 45, Name: Graphic T-shirt, Brand: HuiLi, Details: Cotton t-shirt with graphic print, Score: 3.4255385398864746\n",
      "ID: 51, Name: Polka Dot Dress, Brand: HuiLi, Details: A-line dress with polka dots, Score: 1.547006368637085\n",
      "ID: 6, Name: Z1975 FLARED HIGH-WAIST JEANS, Brand: HuiLi, Details: HIGH-RISE - FLAREFaded high-rise jeans with a five-pocket design. Flared hems. Front zip fly and metal top button fastening., Score: 1.0164390802383423\n",
      "ID: 12, Name: HIGH-RISE SKINNY SCULPT TRF JEANS, Brand: HuiLi, Details: HIGH WAIST - SKINNY - ANKLE LENGTHHigh-waist super stretch jeans with a five-pocket design. Front zip fly and metal top button fastening., Score: 0.9937466382980347\n",
      "ID: 11, Name: Z1975 FLARED HIGH-WAIST JEANS, Brand: HuiLi, Details: HIGH-WAIST - FLAREFaded high-waist jeans with a five-pocket design. Ironed crease detail on the front. Flared hems. Front zip fly and metal top button fastening., Score: 0.8939989805221558\n",
      "ID: 10, Name: Z1975 HIGH-RISE STRAIGHT JEANS, Brand: HuiLi, Details: HIGH-WAIST - STRAIGHT - RIGIDHigh-waist jeans with a five-pocket design. Faded effect. Ironed crease detail on the front. Straight leg. Front zip fly and top button fastening., Score: 0.8764134049415588\n",
      "ID: 15, Name: Z1975 HIGH-WAIST BAGGY PAPERBAG JEANS, Brand: HuiLi, Details: HIGH-RISE - BAGGY PAPERBAG - CARROT LEG - ANKLE - RIGIDHigh-waist jeans with an elasticated waistband and gathered details. Featuring front pockets, rear patch pockets, turn-up hems and a zip and button fastening at the front., Score: 0.770399808883667\n",
      "ID: 25, Name: 100% FEATHER FILL PUFFER JACKET, Brand: HuiLi, Details: Puffer jacket made of shiny finish technical fabric. Fill is a blend of 80% down and 20% feathers. High collar with adjustable hood and long sleeves with elastic cuffs. Welt pockets at hip and interior pocket. Adjustable hem with side elastics. Front zip closure., Score: 0.7628924250602722\n",
      "=====================================================\n",
      "Similar products: [{'id': 55, 'name': 'Casual Shirt', 'brand': 'HuiLi', 'details': 'Cotton shirt with button-down collar', 'score': 3.6248295307159424}, {'id': 35, 'name': 'Casual T-shirt', 'brand': 'HuiLi', 'details': 'Cotton t-shirt with round neck', 'score': 3.4255385398864746}, {'id': 45, 'name': 'Graphic T-shirt', 'brand': 'HuiLi', 'details': 'Cotton t-shirt with graphic print', 'score': 3.4255385398864746}, {'id': 51, 'name': 'Polka Dot Dress', 'brand': 'HuiLi', 'details': 'A-line dress with polka dots', 'score': 1.547006368637085}, {'id': 6, 'name': 'Z1975 FLARED HIGH-WAIST JEANS', 'brand': 'HuiLi', 'details': 'HIGH-RISE - FLAREFaded high-rise jeans with a five-pocket design. Flared hems. Front zip fly and metal top button fastening.', 'score': 1.0164390802383423}, {'id': 12, 'name': 'HIGH-RISE SKINNY SCULPT TRF JEANS', 'brand': 'HuiLi', 'details': 'HIGH WAIST - SKINNY - ANKLE LENGTHHigh-waist super stretch jeans with a five-pocket design. Front zip fly and metal top button fastening.', 'score': 0.9937466382980347}, {'id': 11, 'name': 'Z1975 FLARED HIGH-WAIST JEANS', 'brand': 'HuiLi', 'details': 'HIGH-WAIST - FLAREFaded high-waist jeans with a five-pocket design. Ironed crease detail on the front. Flared hems. Front zip fly and metal top button fastening.', 'score': 0.8939989805221558}, {'id': 10, 'name': 'Z1975 HIGH-RISE STRAIGHT JEANS', 'brand': 'HuiLi', 'details': 'HIGH-WAIST - STRAIGHT - RIGIDHigh-waist jeans with a five-pocket design. Faded effect. Ironed crease detail on the front. Straight leg. Front zip fly and top button fastening.', 'score': 0.8764134049415588}, {'id': 15, 'name': 'Z1975 HIGH-WAIST BAGGY PAPERBAG JEANS', 'brand': 'HuiLi', 'details': 'HIGH-RISE - BAGGY PAPERBAG - CARROT LEG - ANKLE - RIGIDHigh-waist jeans with an elasticated waistband and gathered details. Featuring front pockets, rear patch pockets, turn-up hems and a zip and button fastening at the front.', 'score': 0.770399808883667}, {'id': 25, 'name': '100% FEATHER FILL PUFFER JACKET', 'brand': 'HuiLi', 'details': 'Puffer jacket made of shiny finish technical fabric. Fill is a blend of 80% down and 20% feathers. High collar with adjustable hood and long sleeves with elastic cuffs. Welt pockets at hip and interior pocket. Adjustable hem with side elastics. Front zip closure.', 'score': 0.7628924250602722}]\n",
      "=====================================================\n",
      "Formatted product data: Casual Shirt (ID: 55, Brand: HuiLi, Details: Cotton shirt with button-down collar), Casual T-shirt (ID: 35, Brand: HuiLi, Details: Cotton t-shirt with round neck), Graphic T-shirt (ID: 45, Brand: HuiLi, Details: Cotton t-shirt with graphic print), Polka Dot Dress (ID: 51, Brand: HuiLi, Details: A-line dress with polka dots), Z1975 FLARED HIGH-WAIST JEANS (ID: 6, Brand: HuiLi, Details: HIGH-RISE - FLAREFaded high-rise jeans with a five-pocket design. Flared hems. Front zip fly and metal top button fastening.), HIGH-RISE SKINNY SCULPT TRF JEANS (ID: 12, Brand: HuiLi, Details: HIGH WAIST - SKINNY - ANKLE LENGTHHigh-waist super stretch jeans with a five-pocket design. Front zip fly and metal top button fastening.), Z1975 FLARED HIGH-WAIST JEANS (ID: 11, Brand: HuiLi, Details: HIGH-WAIST - FLAREFaded high-waist jeans with a five-pocket design. Ironed crease detail on the front. Flared hems. Front zip fly and metal top button fastening.), Z1975 HIGH-RISE STRAIGHT JEANS (ID: 10, Brand: HuiLi, Details: HIGH-WAIST - STRAIGHT - RIGIDHigh-waist jeans with a five-pocket design. Faded effect. Ironed crease detail on the front. Straight leg. Front zip fly and top button fastening.), Z1975 HIGH-WAIST BAGGY PAPERBAG JEANS (ID: 15, Brand: HuiLi, Details: HIGH-RISE - BAGGY PAPERBAG - CARROT LEG - ANKLE - RIGIDHigh-waist jeans with an elasticated waistband and gathered details. Featuring front pockets, rear patch pockets, turn-up hems and a zip and button fastening at the front.), 100% FEATHER FILL PUFFER JACKET (ID: 25, Brand: HuiLi, Details: Puffer jacket made of shiny finish technical fabric. Fill is a blend of 80% down and 20% feathers. High collar with adjustable hood and long sleeves with elastic cuffs. Welt pockets at hip and interior pocket. Adjustable hem with side elastics. Front zip closure.)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Received unsupported message type for Ollama.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 31\u001B[0m\n\u001B[1;32m     28\u001B[0m user_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mactually, I\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mm looking for a white shirt with brand huili\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m detect_topic(user_input) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_topic\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 31\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43msearch_and_generate_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=====================================================\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28mprint\u001B[39m(result)\n",
      "Cell \u001B[0;32mIn[32], line 18\u001B[0m, in \u001B[0;36msearch_and_generate_response\u001B[0;34m(user_query)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFormatted product data: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mproduct_data\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# 使用自定义 OpenAI 类生成响应\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# response = generate_response(llm, user_query, product_data)\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mllm_json_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproduct_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# 返回生成的响应和产品数据（JSON 格式）\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m: response,\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproducts\u001B[39m\u001B[38;5;124m\"\u001B[39m: similar_products  \u001B[38;5;66;03m# 这里返回完整的产品信息\u001B[39;00m\n\u001B[1;32m     25\u001B[0m }\n",
      "Cell \u001B[0;32mIn[30], line 27\u001B[0m, in \u001B[0;36mgenerate_response\u001B[0;34m(openai_model, input_query, product_data)\u001B[0m\n\u001B[1;32m     12\u001B[0m     combined_prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124mUser is searching for clothing. The input query is: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;124mThe following product data matches the query:\u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124mGenerate a response to suggest clothing.\u001B[39m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(input_query, product_data)\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;66;03m# messages = [\u001B[39;00m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;66;03m#     {\"role\": \"system\", \"content\": \"You are a helpful fashion assistant\"},\u001B[39;00m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;66;03m#     {\"role\": \"user\", \"content\": combined_prompt}\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m# Ollama API expects a single string for the prompt\u001B[39;00m\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;66;03m# OllamaPrompt = f\"You are a helpful fashion assistant.\\n{combined_prompt}\"\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mopenai_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcombined_prompt\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# 修改为传递messages\u001B[39;00m\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;66;03m# 输出响应内容以检查是否为有效的 JSON\u001B[39;00m\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRaw API Response:\u001B[39m\u001B[38;5;124m\"\u001B[39m, response)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:641\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    639\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    640\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 641\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    642\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    643\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[1;32m    644\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    645\u001B[0m ]\n\u001B[1;32m    646\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:631\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    630\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 631\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    633\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    636\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    637\u001B[0m         )\n\u001B[1;32m    638\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    639\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:850\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    848\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    849\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 850\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    852\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    853\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    854\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_ollama/chat_models.py:644\u001B[0m, in \u001B[0;36mChatOllama._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    637\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate\u001B[39m(\n\u001B[1;32m    638\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    639\u001B[0m     messages: List[BaseMessage],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    642\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    643\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatResult:\n\u001B[0;32m--> 644\u001B[0m     final_chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_chat_stream_with_aggregation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    645\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    646\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    647\u001B[0m     generation_info \u001B[38;5;241m=\u001B[39m final_chunk\u001B[38;5;241m.\u001B[39mgeneration_info\n\u001B[1;32m    648\u001B[0m     chat_generation \u001B[38;5;241m=\u001B[39m ChatGeneration(\n\u001B[1;32m    649\u001B[0m         message\u001B[38;5;241m=\u001B[39mAIMessage(\n\u001B[1;32m    650\u001B[0m             content\u001B[38;5;241m=\u001B[39mfinal_chunk\u001B[38;5;241m.\u001B[39mtext,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    654\u001B[0m         generation_info\u001B[38;5;241m=\u001B[39mgeneration_info,\n\u001B[1;32m    655\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_ollama/chat_models.py:545\u001B[0m, in \u001B[0;36mChatOllama._chat_stream_with_aggregation\u001B[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_chat_stream_with_aggregation\u001B[39m(\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    538\u001B[0m     messages: List[BaseMessage],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    542\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    543\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatGenerationChunk:\n\u001B[1;32m    544\u001B[0m     final_chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 545\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_chat_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    547\u001B[0m \u001B[43m            \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mChatGenerationChunk\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    548\u001B[0m \u001B[43m                \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mAIMessageChunk\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    549\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    562\u001B[0m \u001B[43m                \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_ollama/chat_models.py:505\u001B[0m, in \u001B[0;36mChatOllama._create_chat_stream\u001B[0;34m(self, messages, stop, **kwargs)\u001B[0m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_chat_stream\u001B[39m(\n\u001B[1;32m    500\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    501\u001B[0m     messages: List[BaseMessage],\n\u001B[1;32m    502\u001B[0m     stop: Optional[List[\u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    503\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    504\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[Union[Mapping[\u001B[38;5;28mstr\u001B[39m, Any], \u001B[38;5;28mstr\u001B[39m]]:\n\u001B[0;32m--> 505\u001B[0m     ollama_messages \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_messages_to_ollama_messages\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    507\u001B[0m     stop \u001B[38;5;241m=\u001B[39m stop \u001B[38;5;28;01mif\u001B[39;00m stop \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop\n\u001B[1;32m    509\u001B[0m     params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_default_params\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_ollama/chat_models.py:404\u001B[0m, in \u001B[0;36mChatOllama._convert_messages_to_ollama_messages\u001B[0;34m(self, messages)\u001B[0m\n\u001B[1;32m    402\u001B[0m     tool_call_id \u001B[38;5;241m=\u001B[39m message\u001B[38;5;241m.\u001B[39mtool_call_id\n\u001B[1;32m    403\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 404\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived unsupported message type for Ollama.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    406\u001B[0m content \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    407\u001B[0m images \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mValueError\u001B[0m: Received unsupported message type for Ollama."
     ]
    }
   ],
   "source": [
    "def search_and_generate_response(user_query):\n",
    "    # 从 Neo4j 使用全文索引检索匹配的服装\n",
    "    similar_products = retrieve_products_fulltext(user_query)  # 使用用户查询进行全文搜索\n",
    "    print(\"=====================================================\")\n",
    "    print(f\"Similar products: {similar_products}\")\n",
    "\n",
    "    # 格式化产品数据以供提示输入\n",
    "    # product_data = \", \".join([f\"Name: {p['name']} ({p['brand']}, {p['details']})\" for p in similar_products])\n",
    "    product_data = \", \".join([f\"{p['name']} (ID: {p['id']}, Brand: {p['brand']}, Details: {p['details']})\" for p in similar_products])\n",
    "    \n",
    "    # 打印格式化的产品数据以调试\n",
    "    print(\"=====================================================\")\n",
    "    print(f\"Formatted product data: {product_data}\")\n",
    "\n",
    "    # 使用自定义 OpenAI 类生成响应\n",
    "    # response = generate_response(llm, user_query, product_data)\n",
    "\n",
    "    response = generate_response(llm_json_mode, user_query, product_data)\n",
    "    \n",
    "    \n",
    "    # 返回生成的响应和产品数据（JSON 格式）\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"products\": similar_products  # 这里返回完整的产品信息\n",
    "    }\n",
    "\n",
    "# 示例用法\n",
    "user_input = \"actually, I'm looking for a white shirt with brand huili\"\n",
    "\n",
    "if detect_topic(user_input) == \"on_topic\":\n",
    "    result = search_and_generate_response(user_input)\n",
    "    print(\"=====================================================\")\n",
    "    print(result)\n",
    "elif detect_topic(user_input) == \"off_topic\":\n",
    "    print({\"response\": \"It seems your question is off-topic. Do you want to search for clothing?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
