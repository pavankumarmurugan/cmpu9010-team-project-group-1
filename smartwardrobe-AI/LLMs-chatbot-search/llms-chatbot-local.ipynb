{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain neo4j sentence-transformers pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import langchain \n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Ollama LLM with configurable parameters\n",
    "\n",
    "ollama_llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",  # URL of your local Ollama server\n",
    "    model=\"llama3.2:latest\",            # Model name\n",
    "    temperature=0.7,                    # Configurable temperature\n",
    "    num_predict=200                      # Configurable maximum tokens for response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "# read the CSV file\n",
    "data = pd.read_csv('test-data-2.csv')\n",
    "\n",
    "# Replace NaN values with empty strings for Neo4j compatibility\n",
    "data = data.replace({np.nan: ''})\n",
    "\n",
    "# Connect to Neo4j database\n",
    "driver = GraphDatabase.driver(\"neo4j://localhost:7687\", auth=(\"neo4j\", \"test_password\"))\n",
    "\n",
    "# Function to store clothing items in Neo4j\n",
    "def create_clothing_graph(tx, row):\n",
    "    query = (\n",
    "        \"\"\"\n",
    "        MERGE (c:Clothing {id: $id})\n",
    "        SET c += {name: $name, brand: $brand, type: $type, group: $group, details: $details, \n",
    "                price: $price, currency: $currency, color: $color, size: $size, \n",
    "                style: $style, pattern: $pattern, material: $material, occasion: $occasion}\n",
    "        MERGE (b:Brand {name: $brand})\n",
    "        MERGE (t:Type {name: $type})\n",
    "        MERGE (b)-[:MAKES]->(c)\n",
    "        MERGE (c)-[:BELONGS_TO]->(t)\n",
    "        \"\"\"\n",
    "    )\n",
    "    tx.run(query, id=row['id'], name=row['name'], brand=row['brand'], type=row['type'], group=row['group'], \n",
    "           details=row['details'], price=row['price'], currency=row['currency'], color=row['color'], \n",
    "           size=row['size'], style=row['style'], pattern=row['pattern'], material=row['material'], \n",
    "           occasion=row['occasion'])\n",
    "\n",
    "# Apply the clothing data to the database\n",
    "def import_clothing_data(data):\n",
    "    with driver.session() as session:\n",
    "        data.apply(lambda row: session.execute_write(create_clothing_graph, row), axis=1)\n",
    "\n",
    "# delete index\n",
    "def delete_fulltext_index():\n",
    "    with driver.session() as session:\n",
    "        session.run(\"DROP INDEX clothingIndex IF EXISTS\")\n",
    "\n",
    "# create full text index\n",
    "def create_fulltext_index():\n",
    "    with driver.session() as session:\n",
    "        session.run(\"\"\"\n",
    "        CREATE FULLTEXT INDEX clothingIndex\n",
    "        FOR (c:Clothing)\n",
    "        ON EACH [c.id, c.name, c.brand, c.details]\n",
    "        \"\"\")\n",
    "\n",
    "# Calling the Delete Index and Create Index functions after a clothing node has been created\n",
    "delete_fulltext_index()  # Delete the index first (if it exists)\n",
    "create_fulltext_index()  # Then create a new index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Full-Text Search in Neo4j and Response Generation\n",
    "\n",
    "def escape_special_characters(query: str) -> str:\n",
    "    # Escape characters that are special in Lucene query syntax\n",
    "    lucene_special_characters = r'([\\+\\-\\!\\(\\)\\{\\}\\[\\]\\^\\\"\\~\\*\\?\\:\\\\\\/])'\n",
    "    return re.sub(lucene_special_characters, r'\\\\\\1', query)\n",
    "\n",
    "def retrieve_products_fulltext(query):\n",
    "\n",
    "    # Ensure the query is a valid string\n",
    "    # Escape special characters in the query\n",
    "    query_string = escape_special_characters(query)\n",
    "\n",
    "    # Retrieve products with error handling\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            CALL db.index.fulltext.queryNodes('clothingIndex', $query)\n",
    "            YIELD node, score\n",
    "            RETURN node.id as id, node.name AS name, node.brand AS brand, node.details AS details, score\n",
    "            ORDER BY score DESC LIMIT 10\n",
    "            \"\"\", {\"query\": query_string})\n",
    "            \n",
    "            products = []\n",
    "            for record in result:\n",
    "                products.append({\n",
    "                    \"id\": record[\"id\"], \n",
    "                    \"name\": record[\"name\"],\n",
    "                    \"brand\": record[\"brand\"],\n",
    "                    \"details\": record[\"details\"],\n",
    "                    \"score\": record[\"score\"]\n",
    "                })\n",
    "        return products\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Neo4j Service unavailable: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Define prompt template with history, input_query, and product_data\n",
    "template = \"\"\"\n",
    "    You are an AI assistant helping users find clothing.\n",
    "    Here is the conversation history so far:\n",
    "    {history}\n",
    "    The user's current query is: \"{input_query}\".\n",
    "    Here are the product details:\n",
    "    {product_data}.\n",
    "    Generate a helpful response to suggest relevant products.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input_query\", \"product_data\"], template=template)\n",
    "\n",
    "# Create summary memory instead of buffer memory\n",
    "memory = ConversationSummaryMemory(llm=ollama_llm)\n",
    "\n",
    "# Create a conversation chain using the prompt and memory\n",
    "conversation_chain = ConversationChain(\n",
    "    llm=ollama_llm, \n",
    "    prompt=prompt, \n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# # Create an LLMChain with the prompt\n",
    "# llm_chain = LLMChain(\n",
    "#     llm=ollama_llm,\n",
    "#     prompt=prompt,\n",
    "#     memory=memory\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for products and generate a response\n",
    "\n",
    "def search_and_generate_response(user_query):\n",
    "    \n",
    "    logging.info(f\"Received user query: {user_query}\")\n",
    "\n",
    "    # Retrieve products from Neo4j\n",
    "    similar_products = retrieve_products_fulltext(user_query)\n",
    "    \n",
    "    if not similar_products:\n",
    "        return {\"response\": \"No matching products found\", \"products\": []}\n",
    "\n",
    "    # Format product data for LLM input\n",
    "    product_data = \", \".join([f\"{p['name']} (ID: {p['id']}, Brand: {p['brand']}, Details: {p['details']})\" for p in similar_products])\n",
    "    \n",
    "    try:\n",
    "        # Generate a response using the conversation chain\n",
    "        response = conversation_chain.predict(\n",
    "            input_query=user_query, \n",
    "            product_data=product_data\n",
    "        )\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM response generation failed: {e}\")\n",
    "        response = \"There was an issue generating the response. Please try again.\"\n",
    "\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"products\": similar_products\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Define prompt template with history, input_query, and product_data\n",
    "template = \"\"\"\n",
    "    You are an AI assistant helping users find clothing.\n",
    "    Here is the conversation history so far:\n",
    "    {history}\n",
    "    The user's current query is: \"{input_query}\".\n",
    "    Here are the product details:\n",
    "    {product_data}.\n",
    "    Generate a helpful response to suggest relevant products.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input_query\", \"product_data\"], template=template)\n",
    "\n",
    "\n",
    "# Initialize memory to store conversation history\n",
    "memory = ConversationSummaryMemory(llm=ollama_llm)\n",
    "\n",
    "# Function to handle session history\n",
    "def get_session_history():\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "# Create a RunnableWithMessageHistory\n",
    "conversation_chain = RunnableWithMessageHistory(\n",
    "    runnable=ollama_llm,\n",
    "    get_session_history=get_session_history\n",
    ")\n",
    "\n",
    "def search_and_generate_response(user_query):\n",
    "    logging.info(f\"Received user query: {user_query}\")\n",
    "\n",
    "    # Retrieve products from Neo4j\n",
    "    similar_products = retrieve_products_fulltext(user_query)\n",
    "\n",
    "    if not similar_products:\n",
    "        return {\"response\": \"No matching products found\", \"products\": []}\n",
    "\n",
    "    # Format product data for LLM input\n",
    "    product_data = \", \".join([f\"{p['name']} (ID: {p['id']}, Brand: {p['brand']}, Details: {p['details']})\" for p in similar_products])\n",
    "\n",
    "    try:\n",
    "        # Generate a response using the new chain\n",
    "        response = conversation_chain.invoke({\n",
    "            \"input_query\": user_query,\n",
    "            \"product_data\": product_data\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM response generation failed: {e}\")\n",
    "        response = \"There was an issue generating the response. Please try again.\"\n",
    "\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"products\": similar_products\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.schema import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Define prompt template with history, input_query, and product_data\n",
    "template = \"\"\"\n",
    "    You are an AI assistant helping users find clothing.\n",
    "    Here is the conversation history so far:\n",
    "    {history}\n",
    "    The user's current query is: \"{input_query}\".\n",
    "    Here are the product details:\n",
    "    {product_data}.\n",
    "    Generate a helpful response to suggest relevant products.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input_query\", \"product_data\"], template=template)\n",
    "\n",
    "# Initialize memory to store conversation history\n",
    "memory = ConversationSummaryMemory(llm=ollama_llm)\n",
    "\n",
    "# Function to handle session history, converting it to message objects\n",
    "def get_session_history():\n",
    "    # Load the memory variables which returns history as a string\n",
    "    conversation_history = memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "    # Convert the string history into a list of message objects for the LLM\n",
    "    messages = []\n",
    "    if conversation_history:\n",
    "        history_lines = conversation_history.split(\"\\n\")\n",
    "        for line in history_lines:\n",
    "            if \"User:\" in line:\n",
    "                messages.append(HumanMessage(content=line.replace(\"User:\", \"\").strip()))\n",
    "            elif \"AI:\" in line:\n",
    "                messages.append(AIMessage(content=line.replace(\"AI:\", \"\").strip()))\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Create a RunnableWithMessageHistory\n",
    "conversation_chain = RunnableWithMessageHistory(\n",
    "    runnable=ollama_llm,\n",
    "    get_session_history=get_session_history\n",
    ")\n",
    "\n",
    "def search_and_generate_response(user_query):\n",
    "    logging.info(f\"Received user query: {user_query}\")\n",
    "\n",
    "    # Retrieve products from Neo4j\n",
    "    similar_products = retrieve_products_fulltext(user_query)\n",
    "\n",
    "    if not similar_products:\n",
    "        return {\"response\": \"No matching products found\", \"products\": []}\n",
    "\n",
    "    # Format product data for LLM input\n",
    "    product_data = \", \".join([f\"{p['name']} (ID: {p['id']}, Brand: {p['brand']}, Details: {p['details']})\" for p in similar_products])\n",
    "\n",
    "    try:\n",
    "        # Generate a response using the new chain\n",
    "        response = conversation_chain.invoke({\n",
    "            \"input_query\": user_query,\n",
    "            \"product_data\": product_data\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM response generation failed: {e}\")\n",
    "        response = \"There was an issue generating the response. Please try again.\"\n",
    "\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"products\": similar_products\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.schema import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Define prompt template with history, input_query, and product_data\n",
    "template = \"\"\"\n",
    "    You are an AI assistant helping users find clothing.\n",
    "    Here is the conversation history so far:\n",
    "    {history}\n",
    "    The user's current query is: \"{input_query}\".\n",
    "    Here are the product details:\n",
    "    {product_data}.\n",
    "    Generate a helpful response to suggest relevant products.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input_query\", \"product_data\"], template=template)\n",
    "\n",
    "# Initialize memory to store conversation history\n",
    "memory = ConversationSummaryMemory(llm=ollama_llm)\n",
    "\n",
    "# Custom wrapper class to hold the messages with a 'messages' attribute\n",
    "class MessageHistory:\n",
    "    def __init__(self, messages):\n",
    "        self.messages = messages\n",
    "\n",
    "# Function to handle session history and wrap it in a MessageHistory object\n",
    "def get_session_history():\n",
    "    # Load the memory variables which returns history as a string\n",
    "    conversation_history = memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "    # Convert the string history into a list of message objects for the LLM\n",
    "    messages = []\n",
    "    if conversation_history:\n",
    "        history_lines = conversation_history.split(\"\\n\")\n",
    "        for line in history_lines:\n",
    "            if \"User:\" in line:\n",
    "                messages.append(HumanMessage(content=line.replace(\"User:\", \"\").strip()))\n",
    "            elif \"AI:\" in line:\n",
    "                messages.append(AIMessage(content=line.replace(\"AI:\", \"\").strip()))\n",
    "    \n",
    "    # Wrap the list of messages in a MessageHistory object\n",
    "    return MessageHistory(messages)\n",
    "\n",
    "# Create a RunnableWithMessageHistory\n",
    "conversation_chain = RunnableWithMessageHistory(\n",
    "    runnable=ollama_llm,\n",
    "    get_session_history=get_session_history\n",
    ")\n",
    "\n",
    "def search_and_generate_response(user_query):\n",
    "    logging.info(f\"Received user query: {user_query}\")\n",
    "\n",
    "    # Retrieve products from Neo4j\n",
    "    similar_products = retrieve_products_fulltext(user_query)\n",
    "\n",
    "    if not similar_products:\n",
    "        return {\"response\": \"No matching products found\", \"products\": []}\n",
    "\n",
    "    # Format product data for LLM input\n",
    "    product_data = \", \".join([f\"{p['name']} (ID: {p['id']}, Brand: {p['brand']}, Details: {p['details']})\" for p in similar_products])\n",
    "\n",
    "    try:\n",
    "        # Prepare the input for the conversation chain\n",
    "        history = get_session_history().messages\n",
    "        \n",
    "        # Generate a response using the new chain\n",
    "        response = conversation_chain.invoke({\n",
    "            \"history\": history,  # Ensure history is passed\n",
    "            \"input_query\": user_query,\n",
    "            \"product_data\": product_data\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM response generation failed: {e}\")\n",
    "        response = \"There was an issue generating the response. Please try again.\"\n",
    "\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"products\": similar_products\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m\n\u001b[1;32m     12\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template)\n\u001b[1;32m     14\u001b[0m chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     15\u001b[0m         {\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: retrieve_products_fulltext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat are some good shoes for running?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_core/runnables/base.py:3018\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3016\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3018\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3020\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_core/runnables/base.py:3721\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   3716\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3717\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3718\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[1;32m   3719\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3720\u001b[0m         ]\n\u001b[0;32m-> 3721\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3722\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_core/runnables/base.py:3705\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[0;34m(step, input, config, key)\u001b[0m\n\u001b[1;32m   3703\u001b[0m context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   3704\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m-> 3705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3707\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3709\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_core/runnables/base.py:4698\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4684\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[1;32m   4685\u001b[0m \n\u001b[1;32m   4686\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4695\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[1;32m   4696\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4699\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4700\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4701\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4702\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4704\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   4706\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4708\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_core/runnables/base.py:1924\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1920\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1921\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1922\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1923\u001b[0m         Output,\n\u001b[0;32m-> 1924\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1926\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1927\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1930\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1932\u001b[0m     )\n\u001b[1;32m   1933\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1934\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_core/runnables/config.py:394\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    393\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_core/runnables/base.py:4554\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4552\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   4553\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4554\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   4556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4557\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[1;32m   4558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/site-packages/langchain_core/runnables/config.py:394\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    393\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mretrieve_products_fulltext\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_products_fulltext\u001b[39m(query):\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Ensure the query is a valid string\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Escape special characters in the query\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     query_string \u001b[38;5;241m=\u001b[39m \u001b[43mescape_special_characters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Retrieve products with error handling\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m, in \u001b[0;36mescape_special_characters\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mescape_special_characters\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Escape characters that are special in Lucene query syntax\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     lucene_special_characters \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m~\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m/])\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlucene_special_characters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLMs-Chat-bot/lib/python3.12/re/__init__.py:186\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'set'"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import  RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"\n",
    "    You are an AI assistant helping users find clothing.\n",
    "    The user's current query is: {user_query}.\n",
    "    Here are the product details:\n",
    "    {context}.\n",
    "    Generate a helpful response to suggest relevant products.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "        {\n",
    "            \"context\": retrieve_products_fulltext,\n",
    "            \"user_query\": RunnablePassthrough(),\n",
    "        }\n",
    "    | prompt\n",
    "    | ollama_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(input={\"user_query\": \"What are some good shoes for running?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_and_generate_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm looking for a white shirt with brand Huili\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detect_topic(user_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_topic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msearch_and_generate_response\u001b[49m(user_input)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m detect_topic(user_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff_topic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'search_and_generate_response' is not defined"
     ]
    }
   ],
   "source": [
    "# topic detection \n",
    "on_topic_keywords = ['jeans', 'dress', 'shirt', 'pants', 'clothing', 'size', 'color', 'fashion', 'outfit', 'trousers', 'jacket']\n",
    "off_topic_keywords = ['weather', 'news', 'movie', 'sports', 'politics', 'celebrity']\n",
    "\n",
    "def detect_topic(user_input):\n",
    "    if any(word in user_input.lower() for word in on_topic_keywords):\n",
    "        return \"on_topic\"\n",
    "    elif any(word in user_input.lower() for word in off_topic_keywords):\n",
    "        return \"off_topic\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Example usage\n",
    "user_input = \"I'm looking for a white shirt with brand Huili\"\n",
    "\n",
    "if detect_topic(user_input) == \"on_topic\":\n",
    "    result = search_and_generate_response(user_input)\n",
    "    print(result)\n",
    "elif detect_topic(user_input) == \"off_topic\":\n",
    "    print({\"response\": \"It seems your question is off-topic. Do you want to search for clothing?\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-Chat-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
