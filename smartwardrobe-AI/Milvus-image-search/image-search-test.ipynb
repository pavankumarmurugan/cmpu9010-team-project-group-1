{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymilvus torch gdown torchvision tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "\n",
    "# Download the dataset\n",
    "url = 'https://drive.google.com/uc?id=1OYDHLEy992qu5C4C8HV5uDIkOWRTAR1_'\n",
    "output = './paintings.zip'\n",
    "gdown.download(url, output)\n",
    "\n",
    "with zipfile.ZipFile(\"./paintings.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./paintings\") # Extract the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "# COLLECTION_NAME = 'image_search'  # Collection name\n",
    "\n",
    "# Define the collection name\n",
    "COLLECTION_NAME = 'image_search_test'  # Collection name\n",
    "\n",
    "DIMENSION = 2048  # Embedding vector size in this example\n",
    "MILVUS_HOST = \"localhost\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "# TOP_K = 3\n",
    "TOP_K = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the Milvus instance using the provided URI.\n",
    "from pymilvus import connections\n",
    "connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the collection already exists, drop it.\n",
    "from pymilvus import utility\n",
    "\n",
    "if utility.has_collection(COLLECTION_NAME):\n",
    "    print(f\"Dropping collection: {COLLECTION_NAME}\")\n",
    "    utility.drop_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the collection that holds the ID, the file path of the image, and its embedding.\n",
    "\n",
    "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name='filepath', dtype=DataType.VARCHAR, max_length=200),  # VARCHARS need a maximum length, so for this example they are set to 200 characters\n",
    "    FieldSchema(name='image_embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "schema = CollectionSchema(fields=fields)\n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index on the newly created collection and load it into memory.\n",
    "index_params = {\n",
    "'metric_type':'L2',\n",
    "'index_type':\"IVF_FLAT\",\n",
    "'params':{'nlist': 16384}\n",
    "}\n",
    "collection.create_index(field_name=\"image_embedding\", index_params=index_params)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Inserting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data.\n",
    "import glob\n",
    "\n",
    "# paths = glob.glob('./image-search-test/**/*.jpg', recursive=True)\n",
    "paths = glob.glob('../clothing-images/**/*.jpg', recursive=True)\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data into batches.\n",
    "import torch\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the data.\n",
    "from torchvision import transforms\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting the data.\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def embed(data):\n",
    "    with torch.no_grad():\n",
    "        output = model(torch.stack(data[0])).squeeze()\n",
    "        collection.insert([data[1], output.tolist()])\n",
    "\n",
    "data_batch = [[],[]]\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    im = Image.open(path).convert('RGB')\n",
    "    data_batch[0].append(preprocess(im))\n",
    "    data_batch[1].append(path)\n",
    "    if len(data_batch[0]) % BATCH_SIZE == 0:\n",
    "        embed(data_batch)\n",
    "        data_batch = [[],[]]\n",
    "\n",
    "if len(data_batch[0]) != 0:\n",
    "    embed(data_batch)\n",
    "\n",
    "collection.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Performing the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Define the search paths and the image to search for\n",
    "search_paths = glob.glob('./image-search-test/042/0422106042.jpg', recursive=True)\n",
    "# search_paths = glob.glob('./test.jpeg', recursive=True)\n",
    "len(search_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def embed(data):\n",
    "    with torch.no_grad():\n",
    "        ret = model(torch.stack(data))\n",
    "        # If more than one image, use squeeze\n",
    "        if len(ret) > 1:\n",
    "            return ret.squeeze().tolist()\n",
    "        # Squeeze would remove batch for single image, so using flatten\n",
    "        else:\n",
    "            return torch.flatten(ret, start_dim=1).tolist()\n",
    "\n",
    "data_batch = [[],[]]\n",
    "\n",
    "for path in search_paths:\n",
    "    im = Image.open(path).convert('RGB')\n",
    "    data_batch[0].append(preprocess(im))\n",
    "    data_batch[1].append(path)\n",
    "\n",
    "embeds = embed(data_batch[0])\n",
    "start = time.time()\n",
    "res = collection.search(embeds, anns_field='image_embedding', param={'nprobe': 128}, limit=TOP_K, output_fields=['filepath'])\n",
    "finish = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Define the search paths and the image to search for\n",
    "search_paths = glob.glob('./image-search-test/049/0490113004.jpg', recursive=True)\n",
    "# search_paths = glob.glob('./test.jpeg', recursive=True)\n",
    "len(search_paths)\n",
    "\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def embed(data):\n",
    "    with torch.no_grad():\n",
    "        ret = model(torch.stack(data))\n",
    "        # If more than one image, use squeeze\n",
    "        if len(ret) > 1:\n",
    "            return ret.squeeze().tolist()\n",
    "        # Squeeze would remove batch for single image, so using flatten\n",
    "        else:\n",
    "            return torch.flatten(ret, start_dim=1).tolist()\n",
    "\n",
    "data_batch = [[],[]]\n",
    "\n",
    "for path in search_paths:\n",
    "    im = Image.open(path).convert('RGB')\n",
    "    data_batch[0].append(preprocess(im))\n",
    "    data_batch[1].append(path)\n",
    "\n",
    "embeds = embed(data_batch[0])\n",
    "start = time.time()\n",
    "res = collection.search(embeds, anns_field='image_embedding', param={'nprobe': 128}, limit=TOP_K, output_fields=['filepath'])\n",
    "finish = time.time()\n",
    "\n",
    "f, axarr = plt.subplots(len(data_batch[1]), TOP_K + 1, figsize=(20, 10), squeeze=False)\n",
    "\n",
    "for hits_i, hits in enumerate(res):\n",
    "    axarr[hits_i][0].imshow(Image.open(data_batch[1][hits_i]))\n",
    "    axarr[hits_i][0].set_axis_off()\n",
    "    axarr[hits_i][0].set_title('Search Time: ' + str(finish - start))\n",
    "    for hit_i, hit in enumerate(hits):\n",
    "        axarr[hits_i][hit_i + 1].imshow(Image.open(hit.entity.get('filepath')))\n",
    "        axarr[hits_i][hit_i + 1].set_axis_off()\n",
    "        axarr[hits_i][hit_i + 1].set_title('Distance: ' + str(hit.distance))\n",
    "\n",
    "plt.savefig('search_result.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Define the search paths and the image to search for\n",
    "search_paths = glob.glob('../clothing-images/088/0880060002.jpg', recursive=True)\n",
    "# search_paths = glob.glob('./image-search-test/050/0501406005.jpg', recursive=True)\n",
    "# search_paths = glob.glob('./test.jpeg', recursive=True)\n",
    "len(search_paths)\n",
    "\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def embed(data):\n",
    "    with torch.no_grad():\n",
    "        ret = model(torch.stack(data))\n",
    "        # If more than one image, use squeeze\n",
    "        if len(ret) > 1:\n",
    "            return ret.squeeze().tolist()\n",
    "        # Squeeze would remove batch for single image, so using flatten\n",
    "        else:\n",
    "            return torch.flatten(ret, start_dim=1).tolist()\n",
    "\n",
    "data_batch = [[],[]]\n",
    "\n",
    "for path in search_paths:\n",
    "    im = Image.open(path).convert('RGB')\n",
    "    data_batch[0].append(preprocess(im))\n",
    "    data_batch[1].append(path)\n",
    "\n",
    "embeds = embed(data_batch[0])\n",
    "start = time.time()\n",
    "res = collection.search(embeds, anns_field='image_embedding', param={'nprobe': 128}, limit=TOP_K, output_fields=['filepath'])\n",
    "finish = time.time()\n",
    "\n",
    "import os\n",
    "\n",
    "filename = 'search_result.png'\n",
    "f, axarr = plt.subplots(len(data_batch[1]), TOP_K + 1, figsize=(20, 10), squeeze=False)\n",
    "\n",
    "for hits_i, hits in enumerate(res):\n",
    "    # Show the query image\n",
    "    axarr[hits_i][0].imshow(Image.open(data_batch[1][hits_i]))\n",
    "    axarr[hits_i][0].set_axis_off()\n",
    "    query_image_filename = os.path.basename(data_batch[1][hits_i])  # Get the query image filename\n",
    "    axarr[hits_i][0].set_title(f'Query: {query_image_filename}')\n",
    "    \n",
    "    # Show the found images and their distances\n",
    "    for hit_i, hit in enumerate(hits):\n",
    "        found_image_path = hit.entity.get('filepath')\n",
    "        axarr[hits_i][hit_i + 1].imshow(Image.open(found_image_path))\n",
    "        axarr[hits_i][hit_i + 1].set_axis_off()\n",
    "        found_image_filename = os.path.basename(found_image_path)  # Get the found image filename\n",
    "        axarr[hits_i][hit_i + 1].set_title(f'{found_image_filename}\\nDistance: {hit.distance}')\n",
    "\n",
    "# Save the result figure\n",
    "plt.savefig(filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
